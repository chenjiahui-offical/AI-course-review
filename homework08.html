<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第八节课作业：梯度下降优化算法</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="ai-assistant.css">
    <script src="ai-assistant.js"></script>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link"><i class="fas fa-arrow-left"></i> 返回目录</a>
        
        <h1>第八节课：梯度下降优化算法</h1>
        
        <div class="warning-box">
            <h4><i class="fas fa-tasks"></i> 课堂作业</h4>
            <p>详细介绍三种最具影响力且被广泛引用的梯度下降改进算法：Adam、RMSprop和Momentum。</p>
        </div>
        
        <h2>1. Adam (Adaptive Moment Estimation) 优化器</h2>
        
        <p>Adam算法由Kingma和Ba在2014年提出，是目前深度学习中使用最广泛的优化算法之一，其原始论文被引用超过10万次。</p>
        
        <h3>1.1 算法思想</h3>
        
        <p>Adam结合了动量法（Momentum）和自适应学习率方法（如AdaGrad、RMSprop）的优点，通过计算梯度的一阶矩估计和二阶矩估计来为每个参数动态调整学习率。</p>
        
        <h3>1.2 数学原理</h3>
        
        <p>设目标函数为 $f(\theta)$，其中 $\theta$ 为模型参数，$g_t = \nabla_\theta f(\theta_t)$ 为时刻 $t$ 的梯度。</p>
        
        <div class="info-box">
            <h4>步骤1：计算一阶矩估计（动量项）</h4>
            <p>$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$</p>
        </div>
        
        <div class="info-box">
            <h4>步骤2：计算二阶矩估计（梯度平方的指数移动平均）</h4>
            <p>$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$</p>
        </div>
        
        <div class="info-box">
            <h4>步骤3：偏差修正</h4>
            <p>由于 $m_0 = 0$ 和 $v_0 = 0$，在训练初期会产生偏向零的偏差，需要进行修正：</p>
            <p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$</p>
            <p>$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
        </div>
        
        <div class="highlight-box">
            <h4><i class="fas fa-lightbulb"></i> 步骤4：参数更新</h4>
            <p>$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$</p>
        </div>
        
        <h3>1.3 超参数设置</h3>
        
        <ul>
            <li>$\beta_1$：一阶矩估计的指数衰减率，默认值为 0.9</li>
            <li>$\beta_2$：二阶矩估计的指数衰减率，默认值为 0.999</li>
            <li>$\eta$：学习率，默认值为 0.001</li>
            <li>$\epsilon$：数值稳定性常数，默认值为 $10^{-8}$</li>
        </ul>
        
        <h3>1.4 算法优势</h3>
        
        <ol>
            <li><strong>自适应学习率</strong>：为每个参数自动调整学习率，减少手动调参的需要</li>
            <li><strong>偏差修正</strong>：有效处理训练初期的偏差问题</li>
            <li><strong>稀疏梯度友好</strong>：对稀疏梯度具有良好的处理能力</li>
            <li><strong>收敛稳定</strong>：结合动量和自适应学习率的优点，收敛更加稳定</li>
        </ol>
        
        <h2>2. RMSprop (Root Mean Square Propagation)</h2>
        
        <p>RMSprop由Geoffrey Hinton在其Coursera课程中提出，虽然没有正式发表的论文，但被广泛采用并成为许多深度学习框架的标准优化器。</p>
        
        <h3>2.1 算法思想</h3>
        
        <p>RMSprop解决了AdaGrad算法中学习率单调递减导致训练后期学习率过小的问题。通过使用指数移动平均来限制历史梯度平方的累积，使得学习率能够在训练过程中保持合理的水平。</p>
        
        <h3>2.2 数学原理</h3>
        
        <div class="info-box">
            <h4>步骤1：计算梯度平方的指数移动平均</h4>
            <p>$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g_t^2$</p>
        </div>
        
        <div class="highlight-box">
            <h4><i class="fas fa-lightbulb"></i> 步骤2：参数更新</h4>
            <p>$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t$</p>
            <p>其中：$\gamma$ 为衰减因子（通常为 0.9），$\eta$ 为学习率（通常为 0.001），$\epsilon$ 为防止除零的小常数（通常为 $10^{-8}$）</p>
        </div>
        
        <h3>2.3 与AdaGrad的比较</h3>
        
        <p>AdaGrad的更新公式为：$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} g_t$</p>
        <p>其中 $G_t = \sum_{i=1}^{t} g_i^2$ 是所有历史梯度平方的累积和。</p>
        
        <p>RMSprop通过指数移动平均 $E[g^2]_t$ 替代了累积和 $G_t$，避免了学习率的无限递减。</p>
        
        <h3>2.4 算法优势</h3>
        
        <ol>
            <li><strong>解决学习率衰减问题</strong>：避免了AdaGrad中学习率过度衰减的问题</li>
            <li><strong>适应非平稳目标</strong>：对非平稳目标函数具有良好的适应性</li>
            <li><strong>计算效率高</strong>：相比于二阶方法，计算开销较小</li>
            <li><strong>RNN友好</strong>：在循环神经网络训练中表现优异</li>
        </ol>
        
        <h2>3. Momentum (动量法)</h2>
        
        <p>动量法最早由Polyak在1964年提出，后来Nesterov在1983年提出了加速版本。作为经典的优化方法，其相关工作累计被引用数万次。</p>
        
        <h3>3.1 算法思想</h3>
        
        <p>动量法借鉴了物理学中的动量概念，通过累积历史梯度信息来加速收敛并减少震荡。就像一个球滚下山坡时会积累动量一样，参数更新也会在一致的方向上积累"动量"。</p>
        
        <h3>3.2 标准动量法</h3>
        
        <div class="info-box">
            <h4>数学公式</h4>
            <p>$v_t = \gamma v_{t-1} + \eta g_t$</p>
            <p>$\theta_{t+1} = \theta_t - v_t$</p>
            <p>其中：</p>
            <ul>
                <li>$v_t$：时刻 $t$ 的速度（动量项）</li>
                <li>$\gamma$：动量系数，通常设置为 0.9</li>
                <li>$\eta$：学习率</li>
            </ul>
        </div>
        
        <p><strong>等价形式</strong>：$\theta_{t+1} = \theta_t - (\gamma v_{t-1} + \eta g_t)$</p>
        
        <h3>3.3 Nesterov加速梯度（NAG）</h3>
        
        <p>Nesterov加速梯度是动量法的改进版本，具有"预见性"。</p>
        
        <div class="highlight-box">
            <h4><i class="fas fa-lightbulb"></i> 数学公式</h4>
            <p>$v_t = \gamma v_{t-1} + \eta \nabla f(\theta_t - \gamma v_{t-1})$</p>
            <p>$\theta_{t+1} = \theta_t - v_t$</p>
        </div>
        
        <p><strong>关键思想</strong>：NAG首先根据当前动量 $\gamma v_{t-1}$ 进行一个"预测性"跳跃到 $\theta_t - \gamma v_{t-1}$，然后在该位置计算梯度，最后结合这个梯度和动量进行实际的参数更新。</p>
        
        <h3>3.4 几何直观</h3>
        
        <p>在损失函数的等高线图中：</p>
        <ul>
            <li><strong>标准梯度下降</strong>：严格按照当前位置的负梯度方向移动</li>
            <li><strong>动量法</strong>：考虑历史移动方向，在一致方向上加速，在震荡方向上减速</li>
            <li><strong>NAG</strong>：先"看一眼"前方的梯度情况，然后做出更明智的移动决策</li>
        </ul>
        
        <h3>3.5 算法优势</h3>
        
        <ol>
            <li><strong>加速收敛</strong>：在相关方向上积累速度，加快收敛</li>
            <li><strong>减少震荡</strong>：在不相关方向上减少震荡</li>
            <li><strong>跳出局部最优</strong>：动量可能帮助跳出浅的局部最优解</li>
            <li><strong>简单有效</strong>：实现简单，计算开销小</li>
        </ol>
        
        <h2>4. 三种算法的比较分析</h2>
        
        <h3>4.1 收敛特性比较</h3>
        
        <table>
            <tr>
                <th>算法</th>
                <th>收敛速度</th>
                <th>稳定性</th>
                <th>超参数敏感性</th>
                <th>内存开销</th>
            </tr>
            <tr>
                <td><strong>Momentum</strong></td>
                <td>中等</td>
                <td>中等</td>
                <td>低</td>
                <td>低</td>
            </tr>
            <tr>
                <td><strong>RMSprop</strong></td>
                <td>快</td>
                <td>高</td>
                <td>中等</td>
                <td>中等</td>
            </tr>
            <tr>
                <td><strong>Adam</strong></td>
                <td>快</td>
                <td>高</td>
                <td>低</td>
                <td>高</td>
            </tr>
        </table>
        
        <h3>4.2 适用场景</h3>
        
        <div class="info-box">
            <h4>Momentum适用于</h4>
            <ul>
                <li>目标函数较为平滑的情况</li>
                <li>计算资源受限的场景</li>
                <li>需要简单可靠的优化方案</li>
            </ul>
        </div>
        
        <div class="info-box">
            <h4>RMSprop适用于</h4>
            <ul>
                <li>循环神经网络训练</li>
                <li>非平稳目标函数</li>
                <li>梯度稀疏的情况</li>
            </ul>
        </div>
        
        <div class="info-box">
            <h4>Adam适用于</h4>
            <ul>
                <li>大多数深度学习任务</li>
                <li>需要快速收敛的场景</li>
                <li>超参数调节困难的情况</li>
            </ul>
        </div>
        
        <h3>4.3 实际应用建议</h3>
        
        <ol>
            <li><strong>首选Adam</strong>：在大多数情况下，Adam是一个安全的选择</li>
            <li><strong>考虑RMSprop</strong>：对于RNN或梯度稀疏的问题</li>
            <li><strong>尝试Momentum</strong>：当需要更好的泛化性能或计算资源受限时</li>
        </ol>
        
        <h2>5. 算法实现示例</h2>
        
        <h3>5.1 Adam算法实现</h3>
        
        <pre><code class="language-python">class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {}  # 一阶矩估计
        self.v = {}  # 二阶矩估计
        self.t = 0   # 时间步
  
    def update(self, params, grads):
        self.t += 1
        for key in params:
            if key not in self.m:
                self.m[key] = np.zeros_like(params[key])
                self.v[key] = np.zeros_like(params[key])
          
            # 更新一阶和二阶矩估计
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grads[key]**2
          
            # 偏差修正
            m_hat = self.m[key] / (1 - self.beta1**self.t)
            v_hat = self.v[key] / (1 - self.beta2**self.t)
          
            # 参数更新
            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)</code></pre>
        
        <h3>5.2 RMSprop算法实现</h3>
        
        <pre><code class="language-python">class RMSprop:
    def __init__(self, lr=0.001, gamma=0.9, epsilon=1e-8):
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.v = {}  # 梯度平方的指数移动平均
  
    def update(self, params, grads):
        for key in params:
            if key not in self.v:
                self.v[key] = np.zeros_like(params[key])
          
            # 更新梯度平方的指数移动平均
            self.v[key] = self.gamma * self.v[key] + (1 - self.gamma) * grads[key]**2
          
            # 参数更新
            params[key] -= self.lr * grads[key] / (np.sqrt(self.v[key]) + self.epsilon)</code></pre>
        
        <h3>5.3 Momentum算法实现</h3>
        
        <pre><code class="language-python">class Momentum:
    def __init__(self, lr=0.01, gamma=0.9):
        self.lr = lr
        self.gamma = gamma
        self.v = {}  # 速度（动量项）
  
    def update(self, params, grads):
        for key in params:
            if key not in self.v:
                self.v[key] = np.zeros_like(params[key])
          
            # 更新速度
            self.v[key] = self.gamma * self.v[key] + self.lr * grads[key]
          
            # 参数更新
            params[key] -= self.v[key]</code></pre>
        
        <div class="success-box">
            <h4><i class="fas fa-check-circle"></i> 总结</h4>
            <p>本作业介绍了三种最具影响力的梯度下降改进算法：</p>
            <ol>
                <li><strong>Adam</strong>：结合动量和自适应学习率，是目前最流行的优化算法</li>
                <li><strong>RMSprop</strong>：解决AdaGrad学习率衰减问题，适合非平稳目标函数</li>
                <li><strong>Momentum</strong>：利用历史梯度信息加速收敛，简单有效</li>
            </ol>
        </div>
        
        <div class="navigation-buttons">
            <a href="homework07.html" class="btn-secondary">
                <i class="fas fa-arrow-left"></i> 上一个作业
            </a>
            <a href="index.html" class="btn-primary">
                <i class="fas fa-home"></i> 返回主页
            </a>
            <a href="homework10.html" class="btn-secondary">
                下一个作业 <i class="fas fa-arrow-right"></i>
            </a>
        </div>
    </div>

    <footer>
        <p>高级人工智能课程复习网站 © 2024</p>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$", right: "$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
</body>
</html>
