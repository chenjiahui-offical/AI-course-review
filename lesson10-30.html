<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>10.30课：强化学习折扣因子</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link"><i class="fas fa-arrow-left"></i> 返回目录</a>
        
        <h1>10.30课：强化学习中的折扣因子</h1>
        
        <div class="warning-box">
            <h4><i class="fas fa-tasks"></i> 课堂作业</h4>
            <p>为什么强化学习中需要折扣因子？折扣因子的作用是什么？</p>
        </div>
        
        <h2>1. 折扣因子的数学定义</h2>
        
        <p>在强化学习中，状态价值函数$V(s)$和动作价值函数$Q(s,a)$的递推方程中，折扣因子$\lambda$（或$\gamma$）用于对未来奖励进行加权：</p>
        
        <div class="formula">
            <p><strong>状态价值函数：</strong></p>
            <p>$V(s) = \mathbb{E}\left[R_t + \lambda R_{t+1} + \lambda^2 R_{t+2} + \cdots \mid S_t = s\right]$</p>
        </div>
        
        <div class="formula">
            <p><strong>动作价值函数：</strong></p>
            <p>$Q(s,a) = \mathbb{E}\left[R_t + \lambda R_{t+1} + \lambda^2 R_{t+2} + \cdots \mid S_t = s, A_t = a\right]$</p>
        </div>
        
        <h2>2. 主要原因分析</h2>
        
        <h3>2.1 重新计算/估计路径收益</h3>
        
        <h4>问题背景</h4>
        <p>在无限时间序列的MDP中，如果不对未来奖励进行折扣，可能出现以下问题：</p>
        <ul>
            <li><strong>无限奖励和问题</strong>：当奖励序列不收敛时，总和可能无限大</li>
            <li><strong>数值计算困难</strong>：无限求和在实际计算中不可行</li>
            <li><strong>收敛性问题</strong>：算法可能无法收敛到稳定解</li>
        </ul>
        
        <h4>折扣因子的解决方案</h4>
        
        <div class="highlight-box">
            <h4><i class="fas fa-lightbulb"></i> 几何级数收敛性</h4>
            <p>引入折扣因子$\lambda$后，即使奖励序列有界，无限和也会收敛：</p>
            <p>$\sum_{t=0}^{\infty} \lambda^t R_t \leq R_{\max} \sum_{t=0}^{\infty} \lambda^t = \frac{R_{\max}}{1-\lambda}$</p>
        </div>
        
        <h4>路径收益重新定义</h4>
        <ul>
            <li><strong>原始路径收益</strong>：$G_t = \sum_{k=0}^{\infty} R_{t+k}$</li>
            <li><strong>折扣路径收益</strong>：$G_t = \sum_{k=0}^{\infty} \lambda^k R_{t+k}$</li>
        </ul>
        
        <h4>实际意义</h4>
        <ul>
            <li>近期奖励获得更高权重</li>
            <li>远期奖励影响逐渐减弱</li>
            <li>符合人类决策的直觉偏好</li>
        </ul>
        
        <h3>2.2 修正递推方程</h3>
        
        <h4>Bellman方程的修正</h4>
        
        <div class="warning-box">
            <h4><i class="fas fa-exclamation-triangle"></i> 无折扣情况的问题</h4>
            <p>$V(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + V(s') \right]$</p>
            <p>这种形式可能导致：</p>
            <ul>
                <li>方程无解或存在多个解</li>
                <li>迭代算法不收敛</li>
            